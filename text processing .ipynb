{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello everyone\n"
     ]
    }
   ],
   "source": [
    "print(\"hello everyone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fci3322\\documents\\github\\step\\step\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\FCI3322\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences:\n",
      "['Artificial intelligence and machine learning are shaping the future of technology.', 'These technologies help automate tasks, improve efficiency, and create innovative solutions in various fields.', 'For instance, recommendation systems and virtual assistants rely on these technologies to deliver personalized experiences.']\n",
      "\n",
      "Stemming Example:\n",
      "  Original Word Stemmed Word\n",
      "0   programming      program\n",
      "1    programmer     programm\n",
      "2    programmed      program\n",
      "3      programs      program\n",
      "4  programmatic   programmat\n",
      "\n",
      "Lemmatization Example:\n",
      "  Original Word Lemmatized Word\n",
      "0       running             run\n",
      "1          runs             run\n",
      "2           ran             run\n",
      "3        easily          easily\n",
      "4        better          better\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "mytext = \"\"\"Artificial intelligence and machine learning are shaping the future of technology. \n",
    "These technologies help automate tasks, improve efficiency, and create innovative solutions in various fields. \n",
    "For instance, recommendation systems and virtual assistants rely on these technologies to deliver personalized experiences.\"\"\"\n",
    "\n",
    "my_sentences = sent_tokenize(mytext)\n",
    "print(\"Tokenized Sentences:\")\n",
    "print(my_sentences)\n",
    "\n",
    "print(\"\\nStemming Example:\")\n",
    "porter_stemmer = PorterStemmer()\n",
    "words_to_stem = [\"programming\", \"programmer\", \"programmed\", \"programs\", \"programmatic\"]\n",
    "stemmed_words = [porter_stemmer.stem(word=word) for word in words_to_stem]\n",
    "stem_df = pd.DataFrame({'Original Word': words_to_stem, 'Stemmed Word': stemmed_words})\n",
    "print(stem_df)\n",
    "\n",
    "print(\"\\nLemmatization Example:\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_to_lemmatize = [\"running\", \"runs\", \"ran\", \"easily\", \"better\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word=word, pos='v') for word in words_to_lemmatize]\n",
    "lemmatized_df = pd.DataFrame({'Original Word': words_to_lemmatize, 'Lemmatized Word': lemmatized_words})\n",
    "print(lemmatized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FCI3322\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the previous chapter, we saw examples of some common NLP \\napplications that we might encounter in everyday life.', 'If we were asked to \\nbuild such an application, think about how we would approach doing so at our \\norganization.', 'We would normally walk through the requirements and break the \\nproblem down into several sub-problems, then try to develop a step-by-step \\nprocedure to solve them.', 'Since language processing is involved, we would also\\nlist all the forms of text processing need']\n"
     ]
    }
   ],
   "source": [
    "mytext = \"\"\"In the previous chapter, we saw examples of some common NLP \n",
    "applications that we might encounter in everyday life. If we were asked to \n",
    "build such an application, think about how we would approach doing so at our \n",
    "organization. We would normally walk through the requirements and break the \n",
    "problem down into several sub-problems, then try to develop a step-by-step \n",
    "procedure to solve them. Since language processing is involved, we would also\n",
    "list all the forms of text processing need\n",
    "\"\"\"\n",
    "my_sentences = sent_tokenize(mytext)\n",
    "print(my_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "#init stremmer\n",
    "Porter_Stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_word</th>\n",
       "      <th>stemmed_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>connected</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>connection</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>connections</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>connects</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  original_word stemmed_word\n",
       "0       connect      connect\n",
       "1     connected      connect\n",
       "2    connection      connect\n",
       "3   connections      connect\n",
       "4      connects      connect"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init stemmer\n",
    "porter_stemmer=PorterStemmer()\n",
    "# stem connect variations\n",
    "words=[\"connect\",\"connected\",\"connection\",\"connections\",\"connects\"]\n",
    "stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "\n",
    "stemdf= pd.DataFrame({'original_word': words,'stemmed_word': stemmed_words})\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\FCI3322\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trouble', 'troubling', 'troubled', 'troubles', 'dogs', 'cats'] ['trouble', 'trouble', 'trouble', 'trouble', 'dog', 'cat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#init lemmatizer\n",
    "# init lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words=[\"trouble\",\"troubling\",\"troubled\",\"troubles\",\"dogs\", \"cats\"]\n",
    "lemmatized_words=[lemmatizer.lemmatize(word=word,pos='v') for word in words]\n",
    "print(words, lemmatized_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
