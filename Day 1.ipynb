{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences:\n",
      "['Artificial intelligence and machine learning are shaping the future of technology.', 'These technologies help automate tasks, improve efficiency, and create innovative solutions in various fields.', 'For instance, recommendation systems and virtual assistants rely on these technologies to deliver personalized experiences.']\n",
      "\n",
      "Stemming Example:\n",
      "  Original Word Stemmed Word\n",
      "0   programming      program\n",
      "1    programmer     programm\n",
      "2    programmed      program\n",
      "3      programs      program\n",
      "4  programmatic   programmat\n",
      "\n",
      "Lemmatization Example:\n",
      "  Original Word Lemmatized Word\n",
      "0       running             run\n",
      "1          runs             run\n",
      "2           ran             run\n",
      "3        easily          easily\n",
      "4        better          better\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "mytext = \"\"\"Artificial intelligence and machine learning are shaping the future of technology. \n",
    "These technologies help automate tasks, improve efficiency, and create innovative solutions in various fields. \n",
    "For instance, recommendation systems and virtual assistants rely on these technologies to deliver personalized experiences.\"\"\"\n",
    "\n",
    "my_sentences = sent_tokenize(mytext)\n",
    "print(\"Tokenized Sentences:\")\n",
    "print(my_sentences)\n",
    "\n",
    "print(\"\\nStemming Example:\")\n",
    "porter_stemmer = PorterStemmer()\n",
    "words_to_stem = [\"programming\", \"programmer\", \"programmed\", \"programs\", \"programmatic\"]\n",
    "stemmed_words = [porter_stemmer.stem(word=word) for word in words_to_stem]\n",
    "stem_df = pd.DataFrame({'Original Word': words_to_stem, 'Stemmed Word': stemmed_words})\n",
    "print(stem_df)\n",
    "\n",
    "print(\"\\nLemmatization Example:\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_to_lemmatize = [\"running\", \"runs\", \"ran\", \"easily\", \"better\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word=word, pos='v') for word in words_to_lemmatize]\n",
    "lemmatized_df = pd.DataFrame({'Original Word': words_to_lemmatize, 'Lemmatized Word': lemmatized_words})\n",
    "print(lemmatized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences:\n",
      "['Artificial intelligence and machine learning are shaping the future of technology.', 'These technologies help automate tasks, improve efficiency, and create innovative solutions in various fields.', 'For instance, recommendation systems and virtual assistants rely on these technologies to deliver personalized experiences.']\n",
      "\n",
      "Stemming Example:\n",
      "     Original Word Stemmed Word\n",
      "0       Artificial     artifici\n",
      "1     intelligence     intellig\n",
      "2              and          and\n",
      "3          machine       machin\n",
      "4         learning        learn\n",
      "5              are          are\n",
      "6          shaping        shape\n",
      "7              the          the\n",
      "8           future        futur\n",
      "9               of           of\n",
      "10      technology    technolog\n",
      "11               .            .\n",
      "12           These        these\n",
      "13    technologies    technolog\n",
      "14            help         help\n",
      "15        automate        autom\n",
      "16           tasks         task\n",
      "17               ,            ,\n",
      "18         improve       improv\n",
      "19      efficiency       effici\n",
      "20               ,            ,\n",
      "21             and          and\n",
      "22          create        creat\n",
      "23      innovative        innov\n",
      "24       solutions        solut\n",
      "25              in           in\n",
      "26         various       variou\n",
      "27          fields        field\n",
      "28               .            .\n",
      "29             For          for\n",
      "30        instance      instanc\n",
      "31               ,            ,\n",
      "32  recommendation    recommend\n",
      "33         systems       system\n",
      "34             and          and\n",
      "35         virtual      virtual\n",
      "36      assistants       assist\n",
      "37            rely         reli\n",
      "38              on           on\n",
      "39           these        these\n",
      "40    technologies    technolog\n",
      "41              to           to\n",
      "42         deliver        deliv\n",
      "43    personalized       person\n",
      "44     experiences       experi\n",
      "45               .            .\n",
      "\n",
      "Lemmatization Example:\n",
      "     Original Word Lemmatized Word\n",
      "0       Artificial      Artificial\n",
      "1     intelligence    intelligence\n",
      "2              and             and\n",
      "3          machine         machine\n",
      "4         learning           learn\n",
      "5              are              be\n",
      "6          shaping           shape\n",
      "7              the             the\n",
      "8           future          future\n",
      "9               of              of\n",
      "10      technology      technology\n",
      "11               .               .\n",
      "12           These           These\n",
      "13    technologies    technologies\n",
      "14            help            help\n",
      "15        automate        automate\n",
      "16           tasks            task\n",
      "17               ,               ,\n",
      "18         improve         improve\n",
      "19      efficiency      efficiency\n",
      "20               ,               ,\n",
      "21             and             and\n",
      "22          create          create\n",
      "23      innovative      innovative\n",
      "24       solutions       solutions\n",
      "25              in              in\n",
      "26         various         various\n",
      "27          fields           field\n",
      "28               .               .\n",
      "29             For             For\n",
      "30        instance        instance\n",
      "31               ,               ,\n",
      "32  recommendation  recommendation\n",
      "33         systems         systems\n",
      "34             and             and\n",
      "35         virtual         virtual\n",
      "36      assistants      assistants\n",
      "37            rely            rely\n",
      "38              on              on\n",
      "39           these           these\n",
      "40    technologies    technologies\n",
      "41              to              to\n",
      "42         deliver         deliver\n",
      "43    personalized     personalize\n",
      "44     experiences      experience\n",
      "45               .               .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sample text\n",
    "mytext = \"\"\"Artificial intelligence and machine learning are shaping the future of technology. \n",
    "These technologies help automate tasks, improve efficiency, and create innovative solutions in various fields. \n",
    "For instance, recommendation systems and virtual assistants rely on these technologies to deliver personalized experiences.\"\"\"\n",
    "\n",
    "# Tokenization\n",
    "sentences = sent_tokenize(mytext)\n",
    "print(\"Tokenized Sentences:\")\n",
    "print(sentences)\n",
    "\n",
    "# Tokenize into words\n",
    "words = word_tokenize(mytext)\n",
    "\n",
    "# Stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "stem_df = pd.DataFrame({'Original Word': words, 'Stemmed Word': stemmed_words})\n",
    "print(\"\\nStemming Example:\")\n",
    "print(stem_df)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "lemmatized_df = pd.DataFrame({'Original Word': words, 'Lemmatized Word': lemmatized_words})\n",
    "print(\"\\nLemmatization Example:\")\n",
    "print(lemmatized_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
