{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjDnBYlW/tOrlko4PcsXSe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRAGHATHAPRIYA2589/step/blob/Day1/Day_1_Task_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "def preprocess_text(passage): #defining the function\n",
        "    # Tokenize the passage\n",
        "    words = word_tokenize(passage.lower())  # Convert to lowercase\n",
        "\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
        "\n",
        "\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "\n",
        "passage = \"\"\"\n",
        "The field of data science has transformed the way businesses operate and make decisions.\n",
        "It combines statistical methods, machine learning, and domain expertise to extract\n",
        "valuable insights from data.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "preprocessed_passage = preprocess_text(passage)\n",
        "print(\"Original Passage:\")\n",
        "print(passage)\n",
        "print(\"\\nPreprocessed Passage:\")\n",
        "print(preprocessed_passage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLrkPt_QAMlG",
        "outputId": "a8a5c4ed-dd60-48db-97fd-5defa19e4f13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Passage:\n",
            "\n",
            "The field of data science has transformed the way businesses operate and make decisions.\n",
            "It combines statistical methods, machine learning, and domain expertise to extract\n",
            "valuable insights from data.\n",
            "\n",
            "\n",
            "Preprocessed Passage:\n",
            "field data scienc transform way busi oper make decis combin statist method machin learn domain expertis extract valuabl insight data\n"
          ]
        }
      ]
    }
  ]
}